---
title: 神经网络知识整理
date: 2018-03-13 17:15:38
tags: [神经网络]
categories: [机器学习]
---

# 简介

**人工神经网络**（Artificial Neural Network，即ANN ），是20世纪80 年代以来人工智能领域兴起的研究热点。ANN是由大量的简单处理单元经广泛并行互连形成的一种网络系统。它是对人脑系统的简化、抽象和模拟，具有大脑功能的许多基本特征。

人工神经元之间通过互连形成的网络称为人工神经网络。在ANN中，神经元之间互连方式成为连接模式或连接模型。它不仅决定了神经元网络的互联结构，同时也决定了神经网络的信号处理方式。

一个ANN的神经元模型确定之后，一个神经网络的特性及能力主要取决于其**网络结构与学习方法**。

----------

# 发展历史

　　1943年[1]，心理学家W.S.McCulloch和数理逻辑学家W.Pitts提出了第一个人工神经元模型——**MP模型**。他们通过MP模型提出了神经元的形式化数学描述和网络结构方法，证明了单个神经元能执行逻辑功能，从而开创了人工神经网络研究的时代。

　　1958 年 Rosenblatt[2]在 MP 模型的基础上提出了**感知器模型**(Perceptron)，第一次将人工神经网络理论应用在实际问题中。M.Minsky等仔细分析了以感知器为代表的神经网络系统的功能及局限后，于1969年出版了《Perceptron》一书，指出感知器不能解决高阶谓词问题。他们的论点极大地影响了神经网络的研究，加之当时串行计算机和人工智能所取得的成就，掩盖了发展新型计算机和人工智能新途径的必要性和迫切性，使人工神经网络的研究处于低潮。

　　在此期间，一些人工神经网络的研究者仍然致力于这一研究，提出了**适应谐振理论（ART网）、自组织映射、认知机网络**，同时进行了神经网络数学理论的研究。以上研究为神经网络的研究和发展奠定了基础。

　　1982年，美国加州工学院物理学家J.J.Hopfield[3-4]提出了**Hopfield神经网格模型**，为以后人工神经网络的研究提供了理论基础，同时引入了“计算能量”概念，给出了网络稳定性判断。1984年，他又提出了连续时间Hopfield神经网络模型，为神经计算机的研究做了开拓性的工作，开创了神经网络用于**联想记忆和优化计算**的新途径，有力地推动了神经网络的研究。

　　1985年，又有学者提出了**波尔兹曼模型**，在学习中采用统计热力学模拟退火技术，保证整个系统趋于全局稳定点。

　　1986年Rumelhart等人提出了**并行分布处理理论**以及具有非线性转移函数的**多层前馈网络**(BackPropagation Networks)的误差反向传播算法(Back Propagation)，Rumelhart和McClelland出版了《Parallel distribution processing: explorations in the microstructures of cognition》。迄今，BP算法已被用于解决大量实际问题。

　　1988年，Linsker对感知机网络提出了新的自组织理论，并在Shanon信息论的基础上形成了最大互信息理论，从而点燃了**基于NN的信息应用理论**的光芒。1988年，Broomhead和Lowe用**径向基函数**(Radial basis function, RBF)提出分层网络的设计方法，从而将NN的设计与数值分析和线性适应滤波相挂钩。

　　90年代初，Vapnik等提出了**支持向量机**(Support vector machines, SVM)和VC(Vapnik-Chervonenkis)维数的概念。人工神经网络的研究受到了各个发达国家的重视，美国国会通过决议将1990年1月5日开始的十年定为“脑的十年”，国际研究组织号召它的成员国将“脑的十年”变为全球行为。在日本的“真实世界计算（RWC）”项目中，人工智能的研究成了一个重要的组成部分。

![ANN简史](/images/ANN简史.png)

----------

# 神经网络的三元素

## 神经元模型

神经元模型图解：

![神经元模型工作原理](/images/神经元模型工作原理.jpg)

功能函数f（激活函数）是表示神经元输入与输出之间关系的函数，根据**功能的不同**，可以得到不同的神经元模型。常用的神经元模型有一下几种：

- （1）**阈值型**（Threshold）：这种模型的神经元没有内部状态，激活函数f是一个**阶跃函数**，表示激活值σ和其输出f(σ)之间的关系。波形如下图中（1）。阈值型神经元是最简单的人工神经元，这种二值型神经元，其输入状态取值1或0，分别表示神经元的兴奋与抑制。如σ>0时激活，σ<0时抑制。其中**M-P模型**就属于一种阈值元件模型，它是由美国心理学家Mc Cukkoch和数学家Pitts提出的最早（1943）神经元模型之一。M-P模型是大多数神经网络模型的基础。M-P模型的6个特点： 

	- 每个神经元都是一个**多输入单输出**的信息处理单元； 

	- 神经元输入分兴奋性输入和抑制性输入两种类型； 

	- 神经元具有**空间整合特性和阈值特性**； 

	- 神经元输入与输出间有固定的时滞，主要取决于突触延搁； 

	- 忽略时间整合作用和不应期； 

	- 神经元本身是非时变的，即其突触时延和突触强度均为常数。 


- （2）**分段线性强饱和型**（Linear staturation）：这种模型又称伪线性，其输入/输出之间在一定范围内满足线性关系，一直延续到输出为最大值1为止，但达到最大就不再增大，波形如下图中（2）。

- （3）**S型**（Sigmoid）：这是一种连续的神经元模型，其输出函数也是一个最大输出值的非线性函数，其输出值在某个范围内连续取值，输入输出特性常用S型函数表示，它反映的是神经元的饱和特性，波形如下图中（3）。

- （4）**子阈累积型**（Subthreshold Summation）：这种类型的激活函数也是作为非线性，当产生的激活值超过T值时，该神经元被激活产生一个反响。在线性范围内，系统的反响是线性的，波形如下图中（4）。

![常用的激活函数](/images/常用的激活函数.jpg)


> 从生理学角度看，阈值型最符合人脑神经元的特点，事实上，人脑神经元正式通过电位的高低两种状态来反映该神经元的兴奋与抑制。然而，由于阶跃函数不可微。因此，**实际上更多使用的是与之相仿的S型函数**。

- **常用激活函数**
	- 线性函数 ( Liner Function )
	- 斜面函数 ( Ramp Function )
	- 阈值函数 ( Threshold Function )
	- S形函数 ( Sigmoid Function )
	- 双极S形函数

	> 前两种是线性的，后两种是非线性的。S形函数与双极S形函数都是可导的(导函数是连续函数)，因此适合用在BP神经网络中。（BP算法要求激活函数可导）

参考：

1. [神经网络学习 之 M-P模型](http://blog.csdn.net/u013007900/article/details/50066315)
2. [神经网络基本原理_PPT](https://wenku.baidu.com/view/b6565786cc7931b765ce15c3.html)
3. [人工神经元模型及常见激活函数](http://blog.csdn.net/dcrmg/article/details/73743742)

## 网络结构

神经网络通常被描述为具有层(输入，隐藏或输出层)，其中每层由并行的单元组成。通常同一层不具有连接、两个相邻层完全连接(每一层的每一个神经元到另一层的每个神经元)。

- **感知器**：只有输入层和输出层，是最简单的神经网络结构。

- **多层感知器**：就是在输入层和输出层之间加入隐层。

	> 随着隐层层数的增多，凸域将可以形成任意的形状，因此可以解决任何复杂的分类问题。实际上，Kolmogorov理论指出：**双隐层感知器就足以解决任何复杂的分类问题**。
- ...(具体如下图所示)

![神经网络结构](/images/神经网络结构.jpg)

参考：

1. [史上最好记的神经网络结构速记表（下）](https://www.leiphone.com/news/201710/0hKyVawQLqAuKIm6.html)
2. [25张图让你读懂神经网络架构](http://blog.csdn.net/nicholas_liu2017/article/details/73694666)

## 学习算法

学习算法是ANN的核心问题，神经网络的学习算法有很多种，大体可以分为**有监督学习和无监督学习**（半监督学习是介于两者之间）。另外一类是死记式学习。

- **有监督学习**（Supervised Learning）：从给定的数据集中学习出一个函数, 当新的数据到来时, 可以根据这个函数预测结果, 训练集通常由人工标注。一般需要实现收集样本数据，将数据分为训练集和检验集两部分，以保证所训练的神经网络同时具有**拟合精度和泛化能力**。一般用于**回归和分类**。

- [**无监督学习**](https://www.cnblogs.com/weihuchao/p/6874683.html)（Unsupervised Learning）：相较于监督学习, 没有人工标注。主要用于聚类(clustering)和降维(DimensionReduction)，可作为增强学习和监督学习的预处理。

- **强化学习**（Reinforcement Learning，增强学习）：通过观察通过什么样的动作获得最好的回报, 每个动作都会对环境有所影响, 学习对象通过观察周围的环境进行判断。

- **半监督学习**（Semi-supervised Learning）：介于监督学习和无监督学习。

- **死记式学习**：网络实现设计成能记忆特定的例子，以后当给定有关钙离子的输入信息时，例子便能被回忆起来。死记式学习中网络的权值一旦设计好了就不再变动，因此其学习是一次性的，而不是一个训练过程。


 > **机器学习算法的选择，考虑的指标**：（1）训练的速度（2）内存使用（3）对新数据预测的准确度（4）透明度或可解释性（您对算法做出预测的理由的理解难易程度）

> **训练与学习的区别**，训练函数和学习函数是两个不同的函数，网络设置中两个都有。训练函数求得权值或阈值之后，由学习函数进行调整，然后再由训练函数训练新的权值或阈值，然后再调整，反复下去。

> - **训练函数**确定调整的大算法，**是全局调整权值和阈值，考虑的是整体误差的最小**。训练函数是如何让误差最小的一些算法，如梯度下降，共轭梯度，这里强调算法。

> - **学习函数**决定调整量怎么确定，是**局部调整权值和阈值，考虑的是单个神经元误差的最小**。学习函数是指权值和阈值的调整规则，或者称更新规则。

----------

# 神经网络模型分类

　　神经网络由大量的神经元互相连接而构成，根据神经元的链接方式，神经网络可以分为3大类。

- **前馈神经网络**(Feedforward Neural Networks)

	前馈网络也称前向网络。这种网络只在训练过程会有反馈信号，而在分类过程中数据只能向前传送，直到到达输出层，层间没有向后的反馈信号，因此被称为前馈网络。前馈网络一般不考虑输出与输入在时间上的滞后效应，只表达输出与输入的映射关系；

	感知机( perceptron)与BP神经网络就属于前馈网络。下图是一个3层的前馈神经网络，其中第一层是输入单元，第二层称为隐含层，第三层称为输出层（输入单元不是神经元，因此图中有2层神经元）。

- **反馈神经网络**(Feedback Neural Networks)

	反馈型神经网络是一种从输出到输入具有反馈连接的神经网络，其结构比前馈网络要复杂得多。反馈神经网络的“反馈”体现在当前的（分类）结果会作为一个输入，影响到下一次的（分类）结果，即当前的（分类）结果是受到先前所有的（分类）结果的影响的。

	典型的反馈型神经网络有：Elman网络和Hopfield网络。

- **自组织网络**(SOM ,Self-Organizing Neural Networks)

	自组织神经网络是一种无导师学习网络。它通过自动寻找样本中的内在规律和本质属性，自组织、自适应地改变网络参数与结构。

	基本结构：输入层和竞争层
	
----------

# 神经网络的基本结构

　　神经网络一般由输入层、隐藏层、输出层三部分组成，具体如下:

- **输入层**（Input layer），众多神经元（Neuron）接受大量非线形输入讯息。输入的讯息称为输入向量。

- **隐藏层**（Hidden layer），简称“隐层”，是输入层和输出层之间众多神经元和链接组成的各个层面。如果有多个隐藏层，则意味着多个激活函数。

- **输出层**（Output layer），讯息在神经元链接中传输、分析、权衡，形成输出结果。输出的讯息称为输出向量。


# 人工神经网络研究的局限性

- ANN研究收到脑科学成果的限制。
- ANN缺乏一个完整、成熟的理论体系。
- ANN研究带有浓厚的策略和经验色彩。
- ANN与传统技术的接口不成熟。

相比于传统技术，ANN相当于一个黑箱，内部映射关系并不明确。ANN相比于经典计算方法并非优越，只有当常规方法解决不了或者效果不佳时，ANN方法才能显示出其优越性。尤其是**问题的机理不甚了解后者不能用数学模型表示的系统**，如故障诊断、特征提取和预测等问题，ANN往往是最有力的工具。另一方面，ANN对于**处理大量原始数据而不能用规则或者公式描述的问题**，表现出极大的灵活性和自适应性。

> 总之，在具有成熟的经典技术问题上，ANN不具优势，但是针对难以数学建模的复杂系统，ANN具有极大的灵活性和适应性。

参考文献
[1] McCulloch W S, Pitts W. A logical calculus of the ideas immanent in nervous activity[J]. The bulletin of mathematical biophysics, 1943, 5(4): 115-133.
[2]Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain[J]. Psychological review, 1958, 65(6): 386.
[3] Hopfield J J. Neurons with graded response have collective computational properties like those of two-state neurons[J]. Proceedings of the national academy of sciences, 1984, 81(10): 3088-3092.
[4] Hopfield J J, Tank D W. Computing with neural circuits- A model[J]. Science, 1986, 233(4764): 625-633.