---
title: 数据不平衡分类问题
date: 2018-03-19 11:35:53
tags: [分类]
categories: [机器学习]
---

# 引言
**不平衡分类问题**是指训练样本数量在类间分布不平衡的模式分类问题。在实际应用中，不平衡问题很常见。有些问题其原始数据的分布就存在不平衡，如通过卫星雷达图片检测海面石油油污、监测信用卡非法交易、发掘基因序列中编码信息以及医学数据分类等。

所谓的数据不平衡是指：数据集样本类别极不均衡。不平衡数据的学习即需要在如此分布不均匀的数据集中学习到有用的信息。

----------


# 不平衡分类问题特征及其存在的问题

不平衡分类问题具有一系列传统模式分类方法没有考虑到的特征，从而引发了一系列的传统模式分类难以解决的问题。

## 数据稀缺问题

样本分布的不平衡容易导致稀有类样本的稀缺，具体地说，稀缺包括**绝对稀缺** 和 **相对稀缺**。

- **绝对稀缺**是指稀有类训练样本数量绝对过少,导致该类信息无法通过训练样本充分表示。

> 绝对数据稀缺类的分类错误率要比一般类高出许多。此外,当某类数据过于稀缺时，容易在特征空间中形成小的数据区域，从而引发**小区块(small disjuncts )问题**。由于小区块与噪声数据难以区分，在小区块存在很高的分类错误率。很多分类器为了防止过学习会进行统计显著性（statistical significance）检测，如在决策树中，只有覆盖足够多样本的决策规则和关联规则才能被保留下来。而小区块的数据经常无法顺利通这类显著性检测，另一方面来说，如果降低检测的阈值，又无法有效地去除噪声。

- **相对稀缺**是指稀有类样本本身数量并不过少，但相对大类，占有的比例过小。总样本数量足够多时，相对稀缺并不一定引起分类器性能下降。相反，绝对稀缺导致的稀有样本分布不集中且数量过少才容易引起分类器性能下降。

> 综上可得，对于**相对稀有样本能通过增加总样本数量来减少数据不平衡对分类器性能的影响，而绝对稀缺则难以解决。**

## 噪声问题

噪声数据的存在不可避免，并在一定程度上影响到分类器性能。但是，对不平衡分类问题，噪声数据对稀有类将产生更大的影响。**稀有类的抗噪能力较弱，并且分类器难以区分稀有类样本和噪声数据**。

> 对于不平衡数据的噪声问题存在很大的困难，噪声会影响分类器的性能，但是由于噪声和稀有类难以区分，很难在保留稀有类的情况下去除噪声。

## 决策面偏移问题

传统的模式分类方法，大都建立在训练样本数量均衡的前提下。当用于解决不平衡分类问题时，它们的分类性能往往有不同程度的下降。

- **基于特征空间决策面进行类别划分的分类器**，如支持向量机，目标在于寻找一个最优的决策面。为了降低噪声数据的影响和防止过学习的产生，最优决策面必须兼顾训练分类准确率和决策面的复杂度，即采用结构风险最小化规则。但是当数据不平衡时，则支持向量的个数也不平衡。在结构最小化原则下，支持向量机会忽略稀有类少量支持向量对结构风险的影响而扩大决策边界，最终导致训练的实际超平面与最优超平面不一致。

- **基于概率估计的分类器**，如贝叶斯分类器,分类准确率依赖于概率分布的准确估计，当稀有类样本过少时，概率估计准确率将远小于大类，稀有类的识别率也因此下降。

- **基于规则的分类器**，如决策树和关联规则分类，需要对规则进行筛选。其中支持度和可信度是规则筛选的重要指标，但是当数据不平衡时，基于上述指标的筛选变得困难且不合理。

## 评价标准问题

分类器评测指标的科学性直接影响着分类器的性能，因为分类器训练的目标是实现最高的评测指标。传统模式分类的评价标准一般是准确率，但是以准确率为评价准则的分类器倾向于降低稀有类的分类效果。且准确率不重视稀有类对分类性能评测的影响。 

----------

# 不平衡分类问题的解决策略

解决不平衡分类问题的策略可以分为两大类。一类是从**训练集**入手，通过改变训练集样本分布，降低不平衡程度。另一类是从**学习算法**入手，根据算法在解决不平衡问题时的缺陷,适当地修改算法使之适应不平衡分类问题。

## 训练集解决不平衡分类问题
### 重采样方法
重采样方法是**上采样**和**下采样**使不平衡的样本分布变得比较平衡，从而提高分类器对稀有类的识别率 

- **上采样**(up-sampling)：通过增加稀有类训练样本数的方法，降低不平衡程度。
	- 最原始的是复制稀有类样本，但是易导致过学习，且对提高稀有类识别率影响不大。
	- 基于启发式的上采样方法，有选择地复制稀有类样本，或者生成新的稀有类样本，如SMOTE。

- **下采样**(down-sampling)：通过舍弃部分大类样本的方法，降低不平衡程度。 

> 虽然重采样在一些数据集上取得了不错的效果，但是这类方法也存在一些缺陷。上采样方法不增加任何新的数据，只是重复或者增加人工生成的稀有类样本，这样增加了训练时间，甚至由于这些重复或是周围生成的新的稀有类样本，使分类器过分注重这些样本，导致过学习。**上采样不能从本质上解决稀有类样本的缺失和数据表示的不充分性**。而下采样在去除大类样本时，容易去除重要的样本信息，虽然有些启发式下采样方法知识去除冗余样本和噪声样本，但多数情况下这类样本只是小部分，因此**下采样方法能够调整的不平衡度相当有限**。

### 训练集划分方法
对训练数据集进行划分，是另一种有效的训练集平衡方法。通过训练集划分得到的子分类器，利用分类器集成的方法获得了良好的效果。具体如下图：

![数据集划分1](/images/数据集划分1.png)

首先根据代价敏感学习的需要，学习一个合理的类别样本分布比例。然后将大类样本随机划分成一系列不相交子集。这些子集的大小由稀有类样本集的数量和预先学习的样本分布比例决定。接下来分别将这些不相交子集跟稀有类样本结合，组成一系列平衡的分类子问题，单独训练成子分类器。最后通过元学习(meta learning)将这些子分类器的输出进一步学习成组合分类器。

该方法子问题采用SVM为子分类器，得到的分类器性能优于上、下采样方法。后有人提出最小最大模块化神经网络模型，利用最小最大化集成规则，有效地将子分类器组合，使组合分类器容易地实现并列学习和增量学习。再后面有人将上述模型推广到支持向量机并提出了“部分对部分”(part vs part)任务分解策略。“部分对部分”任务分解策略可对不平衡两类子问题作进一步分解。这种分解策略可以自由地控制每个子问题的规模和平衡度，并且可以根据先验知识和训练集样本的分布特征，制定有效的分解规则。实验表明，该方法比代价敏感学习和重采样方法能更好地解决不平衡问题。


## 学习算法解决不平衡分类问题
### 分类器集成

训练集重采样后用多种学习方法分别训练，然后将得到的分类器采用多数投票方法给出预测类别。

Estabrook等人[26]通过计算发现,根据训练集的自然分布得到的分类器不一定具有最好的一般化能力.他们提出通过对原不平衡问题进行重采样,从而构建多个平衡度不同的训练集,训练后采用分类器挑选和偏向正类的原则将各个分类器综合。该方法比单独应用上采样和下采样方法获得了更好的准确率和ROC曲线。

### 代价敏感学习
**代价敏感学习**赋予各个类别不同的错分代价，它能很好地解决不平衡分类问题。在算法层面上解决不平衡数据学习的方法主要是基于代价敏感学习算法(Cost-Sensitive Learning)，代价敏感学习方法的核心要素是**代价矩阵**，我们注意到在实际的应用中不同类型的误分类情况导致的代价是不一样的。

![类代价](/images/类代价.png)

基于以上代价矩阵的分析，代价敏感学习方法主要有以下三种实现方式，分别是：

- **基于学习模型**，着眼于对某一具体学习方法的改造，使之能适应不平衡数据下的学习，研究者们针对不同的学习模型如感知机，支持向量机，决策树，神经网络等分别提出了其代价敏感的版本。以代价敏感的决策树为例，可从三个方面对其进行改进以适应不平衡数据的学习，这三个方面分别是决策阈值的选择方面、分裂标准的选择方面、剪枝方面，这三个方面中都可以将代价矩阵引入，具体实现算法可参考参考文献中的相关文章。
- **基于贝叶斯风险理论**，把代价敏感学习看成是分类结果的一种后处理，按照传统方法学习到一个模型，以实现损失最小为目标对结果进行调整，优化公式如下所示。此方法的优点在于它可以不依赖所用具体的分类器，但是缺点也很明显它要求分类器输出值为概率。

- **基于预处理**，将代价用于权重的调整，使得分类器满足代价敏感的特性，下面讲解一种基于Adaboost的权重更新策略。


### 特征选择方法

特征选择方法对于不平衡分类问题同样具有重要意义。样本数量分布很不平衡时，特征的分布同样会不平衡。尤其在文本分类问题中，在大类中经常出现的特征，也许在稀有类中根本不出现。因此，根据不平衡分类问题的特点，选取最具有区分能力的特征，有利于提高稀有类的识别率。

通过采用特征选择来解决不平衡分类问题主要集中于自然语言处理领域。


参考：
1. [叶志飞, 文益民, 吕宝粮. 不平衡分类问题研究综述[J]. 智能系统学报, 2009, 4(2):148-156.](http://xueshu.baidu.com/s?wd=paperuri%3A%283cf390f69dc06a770d43f916e515997d%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Fkns.cnki.net%2FKCMS%2Fdetail%2Fdetail.aspx%3Ffilename%3Dznxt200902015%26dbname%3DCJFD%26dbcode%3DCJFQ&ie=utf-8&sc_us=460617811908092324)。
2. [不平衡数据下的机器学习方法简介](https://www.jianshu.com/p/3e8b9f2764c8)