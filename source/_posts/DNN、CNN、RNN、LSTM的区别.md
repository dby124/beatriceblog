---
title: DNN、CNN、RNN、LSTM的区别
date: 2018-03-26 18:42:00
tags: [神经网络]
categories: [机器学习]
---

昨天听室友问起DNN与CNN的区别，后来听她解释，但是总感觉理解的不是很准确，所在在网上搜了一些资料，这篇博客主要是根据知乎上的回答，加上自己的理解整理出来的。

广义上来说，NN（或是DNN）确实可以认为包含了CNN、RNN这些具体的变种形式。在实际应用中，所谓的深度神经网络DNN，往往融合了多种已知的结构，包括卷积层或是LSTM单元。但是从狭义上来说，单独的DNN、CNN、RNN及LSTM也可以对比。

## DNN(深度神经网络)

神经网络是基于感知机的扩展，而DNN可以理解为有很多隐藏层的神经网络。多层神经网络和深度神经网络DNN其实也是指的一个东西，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）。

### DNN存在的局限：
- **参数数量膨胀**。由于DNN采用的是全连接的形式，结构中的连接带来了数量级的权值参数，这不仅容易导致过拟合，也容易造成陷入局部最优。

- **局部最优**。随着神经网络的加深，优化函数更容易陷入局部最优，且偏离真正的全局最优，对于有限的训练数据，性能甚至不如浅层网络。

- **梯度消失**。使用sigmoid激活函数（传递函数），在BP反向传播梯度时，梯度会衰减，随着神经网络层数的增加，衰减累积下，到底层时梯度基本为0。

- **无法对时间序列上的变化进行建模**。对于样本的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要。


> DNN的基本介绍可参考：
> 
1. [深度神经网络（DNN）模型与前向传播算法](https://www.cnblogs.com/pinard/p/6418668.html)
2. 神经网络入门：[深层学习为何要“Deep”（上）](https://zhuanlan.zhihu.com/p/22888385)
3. 前馈神经网络引入的先验知识：并行、迭代；[深层学习为何要“Deep”（下）](https://zhuanlan.zhihu.com/p/24245040)较难懂，建议先看完公开课再看该篇文章。

----------

## CNN(卷积神经网络)

主要针对DNN存在的**参数数量膨胀**问题，对于CNN，并不是所有的上下层神经元都能直接相连，而是通过“卷积核”作为中介。同一个卷积核在多有图像内是共享的，图像通过卷积操作仍能保留原先的位置关系。

CNN之所以适合图像识别，正式因为CNN模型限制参数个数并挖掘局部结构的这个特点。

> CNN相关知识可参考：
> 
1. [CNN相关知识总结](http://dingby.site/2018/03/21/CNN%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/)
2. [YJango的卷积神经网络——介绍](https://zhuanlan.zhihu.com/p/27642620)

----------

## RNN(循环神经网络)

针对CNN中**无法对时间序列上的变化进行建模**的局限，为了适应对时序数据的处理，出现了RNN。

在普通的全连接网络或者CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立（这种就是前馈神经网络）。而在RNN中，神经元的输出可以在下一个时间戳直接作用到自身。

（t+1）时刻网络的最终结果O(t+1)是该时刻输入和所有历史共同作用的结果，这就达到了对时间序列建模的目的。

**存在的问题**：RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度，而梯度消失的现象出现时间轴上。

> RNN相关知识可参考：
> 
1. [循环神经网络(RNN)模型与前向反向传播算法](https://www.cnblogs.com/pinard/p/6509630.html)
2. [YJango的循环神经网络——介绍](https://zhuanlan.zhihu.com/p/24720659)

----------

## LSTM(长短时记忆单元)

为了解决RNN中时间上的梯度消失，机器学习领域发展出了**长短时记忆单元LSTM**，通过门的开关实现时间上记忆功能，并防止梯度消失。

> 参考：
> 
1. [[译] 理解 LSTM(Long Short-Term Memory, LSTM) 网络](https://www.cnblogs.com/wangduo/p/6773601.html?utm_source=itdadao&utm_medium=referral)
2. [探索LSTM：基本概念到内部结构](https://zhuanlan.zhihu.com/p/27345523)

----------

## 扩展

深度神经网络中的梯度不稳定性，前面层中的梯度或会消失，或会爆炸。前面层上的梯度是来自于后面层上梯度的乘乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景，如梯度消失和梯度爆炸。

- **梯度爆炸**（exploding gradient）：梯度爆炸就是由于初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了。

	在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。

	网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。

	> 解决梯度爆炸的方法参考：[详解梯度爆炸和梯度消失](https://www.cnblogs.com/DLlearning/p/8177273.html)

- **梯度消失**（vanishing gradient）：前面的层比后面的层梯度变化更小，故变化更慢，从而引起了梯度消失问题。

	因为通常神经网络所用的激活函数是sigmoid函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))。因此两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。

> 因为sigmoid导数最大为1/4，故只有当abs(w)>4时梯度爆炸才可能出现。深度学习中最普遍发生的是梯度消失问题。

**解决方法**：使用**ReLU,maxout**等替代sigmoid。

**ReLU与sigmoid的区别**：（1）sigmoid函数值在[0,1],ReLU函数值在[0,+无穷]，所以sigmoid函数可以描述概率，ReLU适合用来描述实数；（2）sigmoid函数的梯度随着x的增大或减小和消失，而ReLU不会。

----------


参考：

1. [CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)...](https://www.zhihu.com/question/34681168)
2. [机器学习总结（九）：梯度消失（vanishing gradient）与梯度爆炸（exploding gradient）问题](https://blog.csdn.net/cppjava_/article/details/68941436)
