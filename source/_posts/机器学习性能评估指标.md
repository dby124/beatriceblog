---
title: 机器学习性能评估指标
date: 2018-03-07 19:59:53
tags: [机器学习]
categories: [机器学习]
---

# 引言：
**机器学习分为三个阶段**：

- 第一阶段：采用学习算法，通过对训练集进行归纳学习得到分类模型；

- 第二阶段：将已经学习得到的分类模型用于测试集，对测试集中未知类别的实例进行分类。

- 第三阶段：性能评估。显然，通过测试集产生的分类未必是最佳的，这就导致对测试集的分类可能产生错误。而人们希望尽量得到信呢个最佳的分类模型，就是的对分类器性能评价至关重要。只有通过优秀的评价标准才能选择出性能更好的分类器。

在机器学习、数据挖掘、推荐系统完成建模之后，需要对模型的效果做评价。业内目前常常采用的评价指标有准确率(Precision)、召回率(Recall)、F值(F-Measure)等，下图是不同机器学习算法的评价指标。下文讲对其中某些指标做简要介绍。

![machine_learning_performance_evaluation](/images/machine_learning_performance_evaluation.png)

# 二元分类器评估准则

- **混淆矩阵**

	- True Positive(真正, TP)：将正类预测为正类数.
	- True Negative(真负 , TN)：将负类预测为负类数.
	- False Positive(假正, FP)：将负类预测为正类数 → 误报 (Type I error).
	- False Negative(假负 , FN)：将正类预测为负类数 → 漏报 (Type II error).

	![classifier_evaluation](/images/classifier_evaluation.png)

> 注：实际正例数`(p)=TP+FN`；实际负例数`(N)=FP+TN`；实例总数`(C)=P+N`。一个混合矩阵已经能够显示出评价分类器性能的一些必要信息。为了更方便地比较不同分类器的性能，从混合矩阵中总结出下列常用的数字评价标准。

- **准确率**(accuracy)：
	- 模型正确的预测新的或先前未见过的数据的类标号的能力。影响分类器准确率的因素有：训练数据集记录的数目、属性的数目、属性中的信息、测试数据集记录的分布情况等。
	- 定义：正确分类的测试实例个数占测试实例总数的比例：`Accuracy = (TP+TN)/(TP+TN+FP+FN)` 。
	> 在正负样本不平衡的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用acc，即使全部预测成负类（不点击）acc 也有 99% 以上，没有意义。
- **误分率**
	- 评价准则：分类器做出错误分类的概率有多大。
	- 定义：错误分类的测试实例个数占测试实例总数的比例：`Error_rate=1-Accuracy = (FN+FP)/C `。
- **精确率**(precision)，也称查准率：TP/(TP+FP)
	- 模型正确的预测新的或先前未见过的数据的类标号的能力。影响分类器准确率的因素有：训练数据集记录的数目、属性的数目、属性中的信息、测试数据集记录的分布情况等。
	- 定义：正确分类的测试实例个数占测试实例总数的比例：`Accuracy = (TP+TN)/C` 。
- **召回率**(recall)，也称查全率：TP/(TP+FN)
- **F-值（F-Measure）**，精确率和召回率的调和均值：(α^2+1)PR/(α^2)(P+R);当α=1时就是常见的**F1**，F1=2PR/(P+R)=2TP/(2TP+FP+FN)

	> 此处α>0用于衡量查全率对查准率的相对重要性[Van Rijsbergen，1979]。α=1时退化为标准的F1；α>1时，查全率有更大的影响；α<1时查准率有更大影响。

	> 精确率和准确率都高的情况下，F1 值也会高。P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）。 F-Measure是Precision和Recall加权调和平均。

-  **计算复杂度**：决定着算法执行的速度和占用的资源，它依赖于具体的实现细节和软硬件环境。由于数据挖掘的操作对象是海量的数据库，因而空间和时间的复杂度将是非常重要的问题。

-  **速度**：这涉及产生和使用模型的时间花费。

-  **可解释性**：分类结果只有可解释性好，容易理解，才能更好地用于决策支持。

-  **可伸缩性**：一个模型的可伸缩性，使之在给定内存和磁盘空间等可用的系统资源的前提下，算法的运行时间应当随数据库大小线性增加。

-  **稳定性**：一个模型是稳定的，是指它没有随着它所针对数据的变换而过于剧烈变化。

-  **成本**：这涉及预测错误代价所产生的计算花费。

　　使用这些评价标准可以对分类器进行评估，尤其是其中的准确率或误分率，是比较常用的分类器性能评价标准。

　　但是，所有浙西诶性能评价标准都只在一个操作点有效，这个操作点即是选择使得错误率概率最小的点。而且，这些评价标准都有一个共同的弱点，即它们对于类分布的改变显然不够强壮。当测试集中正例和负例的比例发生改变时，它们可能不在具有良好的性能，甚至不被接受。

- **ROC曲线**（Receiver Operating Characteristic）:比如在逻辑回归里面，我们会设一个阈值，大于这个值的为正类，小于这个值为负类。如果我们减小这个阀值，那么更多的样本会被识别为正类。这会提高正类的识别率，但同时也会使得更多的负类被错误识别为正类。为了形象化这一变化，在此引入 ROC ，ROC 曲线可以用于评价一个分类器好坏。ROC 关注两个指标:

	![ROC](/images/ROC.png)


	> 直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正率）和 FP（假正率）间的 trade-off2。

	- ROC曲线是一个二维图形，横轴为FPR，纵轴为TPR，直观的展示FPR与TPR之间的对应关系。
		- （FPR=0，TPR=0）意味着将每一个实例都预测为负例
		- （FPR=1，TPR=1）意味着将每一个实例都预测为正例
		- （FPR=0，TPR=1）为最优分类器点
	- 实际上，通过有限实例产生的ROC曲线实际上是一个阶梯函数，该曲线近似于实例数量接近无限时对应的ROC曲线。
	- 一个优秀分类器对应的ROC曲线应该尽量靠近单位方形的**左上角**。而如果一条ROC曲线沿着将负分类器点和正分类器点连接构成的对角线，则该分类器的预测效果与随机猜测的同样差。
	

- **AUC（Area Under Curve）**:为ROC曲线下的面积，显然这个面积的数值不会大于1。随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是AUC值。简单说：AUC值越大的分类器，正确率越高

	- AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
	- 0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
	- AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
	- AUC<0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC<0.5 的情况。
	> 既然已经这么多评价标准，为什么还要使用**ROC和AUC**呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反）。

> ROC曲线和AUC的优势：不受类分布的影响，适合与评估、比较类分布不平衡的数据集。

- **PR曲线**（Precision-Recall）曲线。 查准率和查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低，而查全率高时，查准率往往偏低。很多情形下，可以根据学习器的预测结果对样例进行排序，排在前面的学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本。按此顺序逐个把样本的作为正例进行预测，则每次可以计算当前的查全率、查准率。以查准率为纵轴、查全率作为横轴作图。就会得到如下图的查准率-查全率曲线，简称“P-R曲线”，显示该曲线的图称为“P-R图”。

	P-R图直观地显示出学习器在样本总体上的查全率、查准率。在进行比较时，如图中A包住C，说明A的查全率、查准率均高于C,A优于C，而对于A、B存在交叉的情况，采用曲线下面积大小衡量性能，面积越大，性能越好，此处A优于B。

	另外一种比较A、B学习器的方法是：**平衡点**（Break-Event Point，简称BEP），它是当查全率等于查准率时的取值，有图可知，此时A的BEP大于B，故A优于B。

	![P-R_figure](/images/P-R_figure.png)

	举个例子（例子来自Paper：Learning from eImbalanced Data）： 
	假设N_c>>P_c（即Negative的数量远远大于Positive的数量），若FP很大，即有很多N的sample被预测为P，因为这里写图片描述，因此FP_rate的值仍然很小（如果利用ROC曲线则会判断其性能很好，但是实际上其性能并不好），但是如果利用PR，因为Precision综合考虑了TP和FP的值，因此**在极度不平衡的数据下（Positive的样本较少），PR曲线可能比ROC曲线更实用**。


[](#多类分类器评估准则)


# 回归评估

- 平均绝对误差（Mean Absolute Error，MAE）又被称为 l1 范数损失（l1-norm loss）

	![mae](/images/mae.png)

- 平均平方误差（Mean Squared Error，MSE）又被称为 l2 范数损失（l2-norm loss）

	![mse](/images/mse.png)

参考资料：

1. 周志华《机器学习》第2章：模型评估与选择
2. [ 机器学习：准确率(Precision)、召回率(Recall)、F值(F-Measure)、ROC曲线、PR曲线](http://blog.csdn.net/quiet_girl/article/details/70830796)
2. [机器学习性能评估指标](http://charleshm.github.io/2016/03/Model-Performance/)
3. [第三章4-分类器性能评价-20140925](https://wenku.baidu.com/view/ce4f70efaf1ffc4ffe47acfe.html)
4. [分类器的分类性能评价指标](http://kns.cnki.net/KXReader/Detail?dbcode=CJFD&filename=GWDZ201108006&UID=WEEvREcwSlJHSldRa1FhdXNXa0hHRXZVTE5rcHBSVE1yMlFBSCtUZ21yTT0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&autoLogin=0)
