---
title: 机器学习性能评估指标
date: 2018-03-07 19:59:53
tags: [机器学习]
categories: [机器学习]
---

# 引言：

在机器学习、数据挖掘、推荐系统完成建模之后，需要对模型的效果做评价。

业内目前常常采用的评价指标有准确率(Precision)、召回率(Recall)、F值(F-Measure)等，下图是不同机器学习算法的评价指标。下文讲对其中某些指标做简要介绍。

![machine_learning_performance_evaluation](/images/machine_learning_performance_evaluation.png)

# 二元分类器评估准则

- **混淆矩阵**
	- True Positive(真正, TP)：将正类预测为正类数.
	- True Negative(真负 , TN)：将负类预测为负类数.
	- False Positive(假正, FP)：将负类预测为正类数 → 误报 (Type I error).
	- False Negative(假负 , FN)：将正类预测为负类数 → 漏报 (Type II error).

	![classifier_evaluation](/images/classifier_evaluation.png)

- **准确率**(accuracy):(TP+TN)/(TP+TN+FP+FN)
	> 在正负样本不平衡的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用acc，即使全部预测成负类（不点击）acc 也有 99% 以上，没有意义。
- **精确率**(precision)，也称查准率：TP/(TP+FP)
- **召回率**(recall)，也称查全率：TP/(TP+FN)
- **F-值（F-Measure）**，精确率和召回率的调和均值：(α^2+1)PR/(α^2)(P+R);当α=1时就是常见的F1，F1=2PR/(P+R)=2TP/(2TP+FP+FN)

	> 此处α>0用于衡量查全率对查准率的相对重要性[Van Rijsbergen，1979]。α=1时退化为标准的F1；α>1时，查全率有更大的影响；α<1时查准率有更大影响。

	> 精确率和准确率都高的情况下，F1 值也会高。P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）。 F-Measure是Precision和Recall加权调和平均。

- **ROC曲线**:比如在逻辑回归里面，我们会设一个阈值，大于这个值的为正类，小于这个值为负类。如果我们减小这个阀值，那么更多的样本会被识别为正类。这会提高正类的识别率，但同时也会使得更多的负类被错误识别为正类。为了形象化这一变化，在此引入 ROC ，ROC 曲线可以用于评价一个分类器好坏。ROC 关注两个指标:

	![ROC](/images/ROC.png)

	> 直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正率）和 FP（假正率）间的 trade-off2。
- **AUC（Area Under Curve）**:为ROC曲线下的面积，显然这个面积的数值不会大于1。随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是AUC值。简单说：AUC值越大的分类器，正确率越高

	- AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
	- 0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
	- AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
	- AUC<0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC<0.5 的情况。
	> 既然已经这么多评价标准，为什么还要使用**ROC和AUC**呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反）。

- **PR曲线**（Precision-Recall）曲线。 查准率和查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低，而查全率高时，查准率往往偏低。很多情形下，可以根据学习器的预测结果对样例进行排序，排在前面的学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本。按此顺序逐个把样本的作为正例进行预测，则每次可以计算当前的查全率、查准率。以查准率为纵轴、查全率作为横轴作图。就会得到如下图的查准率-查全率曲线，简称“P-R曲线”，显示该曲线的图称为“P-R图”。

	P-R图直观地显示出学习器在样本总体上的查全率、查准率。在进行比较时，如图中A包住C，说明A的查全率、查准率均高于C,A优于C，而对于A、B存在交叉的情况，采用曲线下面积大小衡量性能，面积越大，性能越好，此处A优于B。

	另外一种比较A、B学习器的方法是：**平衡点**（Break-Event Point，简称BEP），它是当查全率等于查准率时的取值，有图可知，此时A的BEP大于B，故A优于B。

	![P-R_figure](/images/P-R_figure.png)

	举个例子（例子来自Paper：Learning from eImbalanced Data）： 
	假设N_c>>P_c（即Negative的数量远远大于Positive的数量），若FP很大，即有很多N的sample被预测为P，因为这里写图片描述，因此FP_rate的值仍然很小（如果利用ROC曲线则会判断其性能很好，但是实际上其性能并不好），但是如果利用PR，因为Precision综合考虑了TP和FP的值，因此**在极度不平衡的数据下（Positive的样本较少），PR曲线可能比ROC曲线更实用**。

[](#多类分类器评估准则)


# 回归评估

- 平均绝对误差（Mean Absolute Error，MAE）又被称为 l1 范数损失（l1-norm loss）

	![mae](/images/mae.png)

- 平均平方误差（Mean Squared Error，MSE）又被称为 l2 范数损失（l2-norm loss）

	![mse](/images/mse.png)

参考资料：

1. 周志华《机器学习》第2章：模型评估与选择
2. [ 机器学习：准确率(Precision)、召回率(Recall)、F值(F-Measure)、ROC曲线、PR曲线](http://blog.csdn.net/quiet_girl/article/details/70830796)
2. [机器学习性能评估指标](http://charleshm.github.io/2016/03/Model-Performance/)
