---
title: 机器学习性能评估指标
date: 2018-03-07 19:59:53
tags: [机器学习]
categories: [机器学习]
---

# 引言：

**分类**是数据挖掘三大核心技术( 关联规则、分类、聚类) 之一 ,其实质是产生一个目标函数 *f* , 该函数将输入数据集的属性集 *x* 映射到已经定义的类标签 *y* 上。该目标函数通常也被称为分类模型或**分类器**

**机器学习分为三个阶段**：

- 第一阶段：**学习模型**。采用学习算法，通过对训练集进行归纳学习得到分类模型；

- 第二阶段：**测试模型**。将已经学习得到的分类模型用于测试集，对测试集中未知类别的实例进行分类。

- 第三阶段：**性能评估**。显然，通过测试集产生的分类未必是最佳的，这就导致对测试集的分类可能产生错误。而人们希望尽量得到信呢个最佳的分类模型，就是的对分类器性能评价至关重要。只有通过优秀的评价标准才能选择出性能更好的分类器。

在机器学习、数据挖掘、推荐系统完成建模之后，需要对模型的效果做评价。业内目前常常采用的评价指标有精确率(Precision)、召回率(Recall)、F值(F-Measure)等，下图是不同机器学习算法的评价指标。下文讲对其中某些指标做简要介绍。

![machine_learning_performance_evaluation](/images/machine_learning_performance_evaluation.png)

# 分类器评估准则

## 混淆矩阵
目前 ,分类器性能评价标准很多,其中比较常用的主要有准确度或错误率、查全率、查准率和F1等。为了清楚地认识这些评价标准, 首先介绍一下混淆矩阵。

### 定义

**混淆矩阵**(Confusion matrix)就是用于总结有指导分类结果的矩阵。沿着主对角线上的项表示正确分类的总数，其他非主对角线的项表示分类的错误数。

### 二分类的混淆矩阵

![classifier_evaluation](/images/classifier_evaluation.png)

- True Positive(真正, TP)：将正类预测为正类数.
- True Negative(真负 , TN)：将负类预测为负类数.
- False Positive(假正, FP)：将负类预测为正类数 → 误报 (Type I error).
- False Negative(假负 , FN)：将正类预测为负类数 → 漏报 (Type II error).

> 注：实际正例数`(p)=TP+FN`；实际负例数`(N)=FP+TN`；实例总数`(C)=P+N`。一个混合矩阵已经能够显示出评价分类器性能的一些必要信息。为了更方便地比较不同分类器的性能，从混合矩阵中总结出准确率、精确率、召回率、F-值（F-measure）等。

### 多类分类的混淆矩阵
**定义**：对于一个*m*分的标准分类问题来说，也可以定义如表1所示*m*×*m*的*m*分混淆矩阵和每一个类属的Recall、Precision、F-measure和Accuracy值。

![classifier_evaluation2](/images/classifier_evaluation2.png)

其相应的整个分类器的准确率表达式如下：

![accuracy](/images/accuracy.png)


## 准确率(accuracy)
- **定义**：正确分类的测试实例个数占测试实例总数的比例，用于衡量模型正确的预测新的或先前未见过的数据的类标号的能力。

- **计算公式**：`Accuracy = (TP+TN)/(TP+TN+FP+FN)` 。

> 在**正负样本不平衡**的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用Accuracy，即使全部预测成负类（不点击）Accuracy也有 99% 以上，没有意义。

> **影响分类器准确率的因素有**：训练数据集记录的数目、属性的数目、属性中的信息、测试数据集记录的分布情况等。

##  误分率

- **定义**：错误分类的测试实例个数占测试实例总数的比例，表示分类器做出错误分类的概率有多大。

- **计算公式**：`Error_rate=1-Accuracy = (FN+FP)/(TP+TN+FP+FN) `。

## 精确率(precision)

- **定义**：正确分类的正例个数占分类为正例的实例个数的比例，也称**查准率**。

- **计算公式**：`TP/(TP+FP)`

## 召回率(recall)

- **定义**：正确分类的正例个数占实际正例个数的比例 也称**查全率**。

- **计算公式**：`TP/(TP+FN)`

## F-值（F-Measure）

- **定义**：F-Measure是查全率与查准率加权调和平均，又称为**F-Score**。

- **计算公式**：（1）`F1 = 2PR/(P+R)=2TP/(2TP+FP+FN)`
（2）`F- = (α^2+1)PR/(α^2)(P+R)`

>  此处α>0用于衡量查全率对查准率的相对重要性[Van Rijsbergen，1979]。α=1时退化为标准的F1；α>1时，查全率有更大的影响；α<1时查准率有更大影响。

> 精确率和准确率都高的情况下，F1值也会高。P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们。F1值在实际应用中较常用。

> **上述的评价标准对于类分布的改变较敏感。** 

## **ROC曲线**（Receiver Operating Characteristic）

比如在逻辑回归里面，我们会设一个阈值，大于这个值的为正类，小于这个值为负类。如果我们减小这个阀值，那么更多的样本会被识别为正类。这会提高正类的识别率，但同时也会使得更多的负类被错误识别为正类。为了形象化这一变化，在此引入 ROC ，ROC 曲线可以用于评价一个分类器好坏。ROC 关注两个指标:

![ROC](/images/ROC.png)

- 直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正率）和 FP（假正率）间的 trade-off2。

- ROC曲线是一个二维图形，横轴为FPR，纵轴为TPR，直观的展示FPR与TPR之间的对应关系。
	- （FPR=0，TPR=0）意味着将每一个实例都预测为负例
	- （FPR=1，TPR=1）意味着将每一个实例都预测为正例
	- （FPR=0，TPR=1）为最优分类器点

- 实际上，通过有限实例产生的ROC曲线实际上是一个阶梯函数，该曲线近似于实例数量接近无限时对应的ROC曲线。

- 一个优秀分类器对应的ROC曲线应该尽量靠近单位方形的**左上角**。而如果一条ROC曲线沿着将负分类器点和正分类器点连接构成的对角线，则该分类器的预测效果与随机猜测的同样差。


## AUC（Area Under Curve）

为ROC曲线下的面积，显然这个面积的数值不会大于1。随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是AUC值。简单说：AUC值越大的分类器，正确率越高

- AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
- 0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
- AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
- AUC<0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC<0.5 的情况。
> 既然已经这么多评价标准，为什么还要使用**ROC和AUC**呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反）。

> ROC曲线和AUC的优势：**不受类分布的影响，适合与评估、比较类分布不平衡的数据集**。因此ROC曲线与AUC已被广泛用于医疗决策制定、模式识别和数据挖掘等领域。


## PR曲线（Precision-Recall）曲线

查准率和查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低，而查全率高时，查准率往往偏低。很多情形下，可以根据学习器的预测结果对样例进行排序，排在前面的学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本。按此顺序逐个把样本的作为正例进行预测，则每次可以计算当前的查全率、查准率。以查准率为纵轴、查全率作为横轴作图。就会得到如下图的查准率-查全率曲线，简称“P-R曲线”，显示该曲线的图称为“P-R图”。

P-R图直观地显示出学习器在样本总体上的查全率、查准率。在进行比较时，如图中A包住C，说明A的查全率、查准率均高于C,A优于C，而对于A、B存在交叉的情况，采用曲线下面积大小衡量性能，面积越大，性能越好，此处A优于B。

另外一种比较A、B学习器的方法是：**平衡点**（Break-Event Point，简称BEP），它是当查全率等于查准率时的取值，有图可知，此时A的BEP大于B，故A优于B。

![P-R_figure](/images/P-R_figure.png)

举个例子（例子来自Paper：Learning from eImbalanced Data）： 
假设N_c>>P_c（即Negative的数量远远大于Positive的数量），若FP很大，即有很多N的sample被预测为P，因为这里写图片描述，因此FP_rate的值仍然很小（如果利用ROC曲线则会判断其性能很好，但是实际上其性能并不好），但是如果利用PR，因为Precision综合考虑了TP和FP的值，因此**在极度不平衡的数据下（Positive的样本较少），PR曲线可能比ROC曲线更实用**。

## 其他评估指标

-  **计算复杂度**：决定着算法执行的速度和占用的资源，它依赖于具体的实现细节和软硬件环境。由于数据挖掘的操作对象是海量的数据库，因而空间和时间的复杂度将是非常重要的问题。

-  **速度**：这涉及产生和使用模型的时间花费。

-  **可解释性**：分类结果只有可解释性好，容易理解，才能更好地用于决策支持。

-  **可伸缩性**：一个模型的可伸缩性，使之在给定内存和磁盘空间等可用的系统资源的前提下，算法的运行时间应当随数据库大小线性增加。

-  **稳定性**：一个模型是稳定的，是指它没有随着它所针对数据的变换而过于剧烈变化。

-  **成本**：这涉及预测错误代价所产生的计算花费。

　　使用这些评价标准可以对分类器进行评估，尤其是其中的准确率或误分率，是比较常用的分类器性能评价标准。

　　但是，所有浙西诶性能评价标准都只在一个操作点有效，这个操作点即是选择使得错误率概率最小的点。而且，这些评价标准都有一个共同的弱点，即它们对于类分布的改变显然不够强壮。当测试集中正例和负例的比例发生改变时，它们可能不在具有良好的性能，甚至不被接受。


# 回归评估

- 平均绝对误差（Mean Absolute Error，MAE）又被称为 l1 范数损失（l1-norm loss）

	![mae](/images/mae.png)

- 平均平方误差（Mean Squared Error，MSE）又被称为 l2 范数损失（l2-norm loss）

	![mse](/images/mse.png)

参考资料：

1. 周志华《机器学习》第2章：模型评估与选择
2. [ 机器学习：准确率(Precision)、召回率(Recall)、F值(F-Measure)、ROC曲线、PR曲线](http://blog.csdn.net/quiet_girl/article/details/70830796)
2. [机器学习性能评估指标](http://charleshm.github.io/2016/03/Model-Performance/)
3. [第三章4-分类器性能评价-20140925](https://wenku.baidu.com/view/ce4f70efaf1ffc4ffe47acfe.html)
4. [分类器的分类性能评价指标](http://kns.cnki.net/KXReader/Detail?dbcode=CJFD&filename=GWDZ201108006&UID=WEEvREcwSlJHSldRa1FhdXNXa0hHRXZVTE5rcHBSVE1yMlFBSCtUZ21yTT0%3d%249A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&autoLogin=0)
