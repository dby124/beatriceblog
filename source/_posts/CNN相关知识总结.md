---
title: CNN相关知识总结
date: 2018-03-21 9:15:38
tags: [CNN]
categories: [机器学习]
---


## CNN简介

卷积神经网络（Convolutional Neural Network，CNN）是一种深度的监督学习下的机器学习模型，具有**极强的适应性，善于挖掘数据局部特征，提取全局训练特征和分类**，它的权值共享结构网络使之更类似于生物神经网络，在模式识别各个领域都取得了很好的成果。CNN是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。

CNN节省训练开销的方式是权共享weight sharing，让一组神经元使用相同的权值。

它的三个基本概念为：局部感受野（Local receptive fields），共享权重（Shared weights），和池化（Pooling）。

## CNN背景
1966年，Minisky和Papert在他们的《感知器》一书中提出了上述的感知器的研究瓶颈，指出理论上还不能证明将感知器模型扩展到多层网络是有意义的。这在人工神经网络的历史上书写了极其灰暗的一章。对ANN的研究，始于1890年开始于美国著名心理学家W.James对于人脑结构与功能的研究，半个世纪后W.S.McCulloch和W.A.Pitts提出了M-P模型，之后的1958年Frank Rosenblatt在这个基础上又提出了感知器，此时对ANN的研究正处在升温阶段，《感知器》这本书的出现就刚好为这刚刚燃起的人工神经网络之火泼了一大盆冷水。一时间人们仿佛感觉对以感知器为基础的ANN的研究突然间走到尽头，看不到出路了。于是，几乎所有为ANN提供的研究基金都枯竭了，很多领域的专家纷纷放弃了这方面课题的研究。 

## CNN结构
卷积神经网络CNN的结构一般包含这几个层：

- **卷积层**（Convolutional layer），使用卷积核进行特征提取和特征映射。卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。

- **线性整流层**（Rectified Linear Units layer, ReLU layer），或称激励层、激活层。由于卷积也是一种线性运算，因此需要增加非线性映射。这一层神经的激励函数（Activation function）一般使用线性整流（Rectified Linear Units, ReLU）f(x)=max(0,x)，层次越深，相对于其他的函数效果较好，其他的激活函数还有Sigmod,tanh函数等，但是sigmod和tanh都存在饱和的问题，如上图所示，当x轴上的值较大时，对应的梯度几乎为0，若是利用BP反向传播算法， 可能造成梯度消失的情况，也就学不到东西了。

	在每个卷积层之后，通常会立即应用一个非线性层（或激活层）。其目的是给一个在卷积层中刚经过线性计算操作（只是数组元素依次（element wise）相乘与求和）的系统引入非线性特征。过去，人们用的是像双曲正切和 S 型函数这样的非线性方程，但研究者发现 ReLU 层效果好得多，因为神经网络能够在准确度不发生明显改变的情况下把训练速度提高很多（由于计算效率增加）。它同样能帮助减轻梯度消失的问题——由于梯度以指数方式在层中消失，导致网络较底层的训练速度非常慢。ReLU 层对输入内容的所有值都应用了函数 f(x) = max(0, x)。用基本术语来说，这一层把所有的负激活（negative activation）都变为零。这一层会增加模型乃至整个神经网络的非线性特征，而且不会影响卷积层的感受野。

> 参见 Geoffrey Hinton（即深度学习之父）的论文：Rectified Linear Units Improve Restricted Boltzmann Machines
作者：机器之心https://www.zhihu.com/question/52668301/answer/131573702

- **池化层**（Pooling layer），进行下采样，对特征图稀疏处理，减少数据运算量。通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。
	- 很明显就是减少参数
	- Pooling就有平移不变性（(translation invariant） 

	在几个 ReLU 层之后，程序员也许会选择用一个池化层（pooling layer）。它同时也被叫做下采样（downsampling）层。在这个类别中，也有几种可供选择的层，最受欢迎的就是最大池化（ max-pooling）。它基本上采用了一个过滤器（通常是 2x2 的）和一个同样长度的步幅。然后把它应用到输入内容上，输出过滤器卷积计算的每个子区域中的最大数字。

	池化层还有其他选择，比如平均池化（average pooling）和 L2-norm 池化 。这一层背后的直观推理是：一旦我们知道了原始输入（这里会有一个高激活值）中一个特定的特征，它与其它特征的相对位置就比它的绝对位置更重要。可想而知，这一层大幅减小了输入卷的空间维度（长度和宽度改变了，但深度没变）。这到达了两个主要目的。第一个是权重参数的数目减少到了75%，因此降低了计算成本。第二是它可以控制过拟合（overfitting）。这个术语是指一个模型与训练样本太过匹配了，以至于用于验证和检测组时无法产生出好的结果。出现过拟合的表现是一个模型在训练集能达到 100% 或 99% 的准确度，而在测试数据上却只有50%。

> [CNN(卷积神经网络)是什么？ 作者：机器之心](https://www.zhihu.com/question/52668301/answer/131573702)

- **全连接层**（ Fully-Connected layer）, 通常在CNN的尾部进行重新拟合，减少特征信息的损失。把所有局部特征结合变成全局特征，用来计算最后每一类的得分。

- \* **Dropout 层**：在神经网络有了非常明确的功能。上一节，我们讨论了经过训练后的过拟合问题：训练之后，神经网络的权重与训练样本太过匹配以至于在处理新样本的时候表现平平。Dropout 的概念在本质上非常简单。Dropout 层将「丢弃（drop out）」该层中一个随机的激活参数集，即在前向通过（forward pass）中将这些激活参数集设置为 0。简单如斯。既然如此，这些简单而且似乎不必要且有些反常的过程的好处是什么？在某种程度上，这种机制强制网络变得更加冗余。这里的意思是：该网络将能够为特定的样本提供合适的分类或输出，即使一些激活参数被丢弃。此机制将保证神经网络不会对训练样本「过于匹配」，这将帮助缓解过拟合问题。另外，Dropout 层只能在训练中使用，而不能用于测试过程，这是很重要的一点。

> 参考 Geoffrey Hinton 的论文：Dropout: A Simple Way to Prevent Neural Networks from Overfitting


## CNN的特点

**局部连接、权值共享、池化操作及多层结构**。
CNN能够通过多层非线性变换，从数据中自动学习特征，从而代替手工设计的特征，且深层的结构使它具有很强的表达能力和学习能力。许多研究实验已经表明了CNN结构中深度的重要性。例如从结构来看， AlexNet、VGG、GooleNet及ResNet的一个典型的发展趋势是它们的深度越来越深。在CNN中，通过增加深度从而增加网络的非线性来使它能够更好地拟合目标函数，获得更好的分布式特征。

## CNN应用
- 图像分类
- 人脸识别
- 音频检索
- ECG分析
- 其他：短文本聚类[103]、视觉追踪[104]、图像融合[105]等领域中.

> 参考：周飞燕, 金林鹏, 董军. 卷积神经网络研究综述[J]. 计算机学报, 2017, 40(6):1229-1251.


----------


参考：

1. [详解卷积神经网络(CNN)](http://blog.csdn.net/qq_25762497/article/details/51052861)
2. [深度学习Deep Learning（01）_CNN卷积神经网络](http://blog.csdn.net/u013082989/article/details/53673602)
3. [CNN(卷积神经网络)是什么？有入门简介或文章吗？](https://www.zhihu.com/question/52668301/answer/131573702)

