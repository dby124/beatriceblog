---
title: BP网络详解
date: 2018-03-20 21:57:38
tags: [BP]
categories: [机器学习]
---

## 概述

神经网络是由大量简单的处理单元来模拟真实人脑神经网络的机构和功能以及若干基本特性，是一个高度复杂的非线性自适应动态处理系统。按照连接模式，神经网络模型可分为前馈式神经网络和反馈式神经网络，BP网络属于前馈式。

**BP算法的提出**：Rumelhart，McClelland于1985年提出了BP网络的**误差反向后传**BP(Back Propagation)学习算法。利用输出后的误差来估计输出层的直接前导层的误差，再用这个误差估计更前一层的误差，如此一层一层的反传下去，就获得了所有其他各层的误差估计。

**BP神经网络的提出**：1986年由Rumelhart和McCelland为首的科研小组提出，参见他们发表在Nature上的论文 [Learning representations by back-propagating errors](http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf)。

BP神经网络名字源于网络权值的调整规则采用的是后向传播学习（BP）算法，是一种按误差逆传播算法训练的多层前馈神经网络，是目前应用最广泛的神经网络模型之一。BP神经网络能学习和存贮大量的输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。BP神经网络的**主要思想**就是在确定了网络结构后，通过输入和输出样本集对网络进行训练和学习，以使网络实现给定的输入和输出映射关系。


## BP神经网络基本原理

BP神经网络属于有监督学习
### BP神经网络神经元模型

神经网络神经元模型主要的不同在于传递函数的不同。传递函数是BP网络的重要组成部分，又称激活函数，必须是连续可导的。BP网络常采用的传递函数如下：

- **S型对数函数**：logsig 函数。函数公式：
`logsig(n) = 1 / (1 + exp(-n))`；matlab调用格式：`A=logsig(N,FP)`；使用语法：`net.layers{i}.transferFcn = ‘logsig’`。

> N为Q个S维的输入列向量；FP为功能结构参数（可选）；A为函数返回值，位于区间（0,1）中。 

- **双曲正切S型传递函数**：tansig 函数；函数公式：
`a = tansig(n) = 2/(1+exp(-2*n))-1`；matlab调用格式：`A=tansig(N,FP)`；使用语法：`net.layers{i}.transferFcn = ‘tansig’`。
> 含义与上述相同，不同在于输出区间（-1,1）中(这也是与logsig函数不同的地方)。 


- **线性传递函数**：purelin 函数，调用格式：`A = purelin(N,FP)`
> 含义与上述相同，不同在于其输出等于输入，即A=N。


### 网络拓扑结构

BP网络是典型的多层网络结构，而多层网络可以解决很多单层网络无法解决的问题，比如多层网络可以用来进行非线性分类、做精度极高的函数逼近。为了描述方便，我们选择三层BP网络，网络结构包含一个输入层、一个隐含层和一个输出层，其网络拓扑结构如下图，图中n为输入层神经元数， h为隐含层神经元数，m为输出层神经元数。

![Bp神经网络拓扑结构.png](/images/Bp神经网络拓扑结构.png.png)

- 各层的神经元个数确定：
	- **输入层**：输入层的神经元个数输入训练集的特征向量的维度。
	- **隐含层**

		![隐含层神经元个数](/images/隐含层神经元个数.png)

		> 对于多层前馈网络来说，隐层节点数的确定是成败的关键。若数量太少，则网络所能获取的用以解决问题的信息太少；若数量太多，不仅增加训练时间，更重要的是隐层节点过多还可能出现所谓“**过拟合**”（Overfitting）问题，即测试误差增大导致泛化能力下降，因此合理选择隐层节点数非常重要。关于隐层数及其节点数的选择比较复杂，一般原则是：在能正确反映输入输出关系的基础上，应选用较少的隐层节点数，以使网络结构尽量简单。
	- **输出层**：输出层的神经元个数为输出类别N的log2(N)；或者是N。


### BP神经网络的学习算法
- **学习的过程**：神经网络在外界输入样本的刺激下不断改变网络的连接权值,以使网络的输出不断地接近期望的输出。
	- 信号的正向传播：输入样本－－输入层－－各隐层－－输出层
	- 误差的反向传播：若输出层的实际输出与期望的输出不符，误差以某种形式在各层表示，即修正各层单元的权值，最后当网络输出的误差减少到可接受的程度进行到预先设定的学习次数为止。
- **学习的本质**：对各连接权值的动态调整。
- **学习规则**：权值调整规则，即在学习过程中网络中各神经元的连接权变化所依据的一定的调整规则。


- **BP神经网络常用的训练函数**：
	- **Levenberg-Marquardt算法训练函数**：`trainlm()`；特点：收敛速度和网络精度,对于**中等规模的BP神经网络有最快的收敛速度,是系统默认的算法**。由于其避免了直接计算赫赛矩阵,从而减少了训练中的计算量,但需要较大内存量。
	
	- **梯度下降BP算法函数**：`traingd()`，沿网络性能参数的负梯度方向调整网络的权值和阈值。
	
	- **梯度下降动量BP算法函数**：`traingdm()`，是一种批处理的前馈神经网络训练方法,不但具有更快的收敛速度,而且引入了一个动量项,有效避免了局部最小问题在网络训练中出现。

	- **BFGS准牛顿BP算法函数**：`trainbfg()`；特点：收敛速度介于梯度下降法和牛顿法之间，**适用于小规模数据，具有收敛速度快，精度高等特点**。除了BP网络外，该函数也可以训练任意形式的神经网络，只要它的传递函数对于权值和输入存在导函数即可。[参考](http://blog.csdn.net/acdreamers/article/details/44664941)


	参考：[BP神经网络常用函数汇总](http://xzh2012.blog.163.com/blog/static/114980038201101844232346/)

## BP神经网络的MATLAB实现 

- **前馈BP神经网络构造函数**：`net = feedforwardnet(h)`，其中h为隐含层结点个数，如果是多层隐含层，则`h=[n1,n2,...,nn]`。ni表示各个隐含层对应的结点数。
> 老版本的构造函数是`newff()`，现在使用的是feedforwardnet函数。

- **常用网络配置参数如下**：

	```sh
	net.trainFcn = 'trainbfg';		% 神经网络的训练函数设置
	net.trainParam.epochs=1000;		% 最大迭代次数,允许最大训练步数1000步
	% net.trainParam.max_fail = 10;	% 验证错误个数，默认为6
	net.trainParam.lr = 0.1;		% 学习率
	net.trainparam.goal = 0;		% 神经网络训练的目标误差
	net.trainparam.show;			% 显示中间结果的周期
	```
- 实例

	```sh
	% 注意训练集及标签为traind，trainl，测试集为testd，其中矩阵的行为样本，列表示特征向量。
	dim = length(traind(1,:));  		% 输入向量的维度
	% 创建和训练网络     
	net = feedforwardnet(2*dim+1);   
	net.trainFcn = 'trainbfg';
	net.trainParam.epochs=1000;			%允许最大训练步数1000步
	% net.trainParam.max_fail = 10;		% 默认为6
	% view(net); 						% 查看网络
	net = train(net,traind',trainl');	% 训练网络
	test_out=sim(net,testd');			% 测试集测试网络
	% 结果分析，对数据取整处理
	test_out(test_out>=0.5)=1;
	test_out(test_out<0.5)=0;
	```

## BP神经网络的特点
- **非线性映射能力**：能学习和存贮大量输入-输出模式映射关系，而无需事先了解描述这种映射关系的数学方程。只要能提供足够多的样本模式对供网络进行学习训练，它便能完成由n维输入空间到m维输出空间的非线性映射。
- **自学习和自适应能力**：BP神经网络在训练时，能够通过学习自动提取输出、输出数据间的“合理规则”，并自适应的将学习内容记忆于网络的权值中。即BP神经网络具有高度自学习和自适应的能力。
- **泛化能力**：当向网络输入训练时未曾见过的非样本数据时，网络也能完成由输入空间向输出空间的正确映射。这种能力称为泛化能力。
- **容错能力**：输入样本中带有较大的误差甚至个别错误对网络的输入输出规律影响很小。

## BP网络的局限性

- **需要较长的训练时间**：这主要是由于学习速率太小所造成的，可采用变化的或自适应的学习速率来加以改进。
- **完全不能训练**：这主要表现在网络的麻痹上，通常为了避免这种情况的产生，一是选取较小的初始权值，而是采用较小的学习速率。
- **局部最小值**：这里采用的梯度下降法可能收敛到局部最小值，采用多层网络或较多的神经元，有可能得到更好的结果。

参考：[神经网络学习 之 BP神经网络](http://blog.csdn.net/u013007900/article/details/50118945)